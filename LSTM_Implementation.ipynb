{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxDGxthzp4BC"
      },
      "source": [
        "# Long Short Term Memory Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7nosSYMA5BF",
        "outputId": "f8266ec8-0f53-404d-92ee-9f212d8f0419"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "520/520 [==============================] - 288s 537ms/step - loss: 0.2345 - accuracy: 0.9078 - val_loss: 0.1764 - val_accuracy: 0.9375\n",
            "Epoch 2/5\n",
            "520/520 [==============================] - 237s 456ms/step - loss: 0.0995 - accuracy: 0.9680 - val_loss: 0.1619 - val_accuracy: 0.9341\n",
            "Epoch 3/5\n",
            "520/520 [==============================] - 244s 470ms/step - loss: 0.0647 - accuracy: 0.9804 - val_loss: 0.1931 - val_accuracy: 0.9406\n",
            "Epoch 4/5\n",
            "520/520 [==============================] - 245s 470ms/step - loss: 0.0416 - accuracy: 0.9882 - val_loss: 0.2053 - val_accuracy: 0.9394\n",
            "Epoch 5/5\n",
            "520/520 [==============================] - 244s 470ms/step - loss: 0.0274 - accuracy: 0.9932 - val_loss: 0.2156 - val_accuracy: 0.9397\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x791fa8540880>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "# Download stopwords (only need to run once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load and preprocess the train data\n",
        "train_path = \"/content/train.csv\"\n",
        "train_data = pd.read_csv(train_path)\n",
        "\n",
        "\n",
        "# Combine 'title', 'author', and 'text' columns to create a single input text\n",
        "train_data['input_text'] = train_data['title'].fillna('') + ' ' + train_data['author'].fillna('') + ' ' + train_data['text'].fillna('')\n",
        "\n",
        "# Define stopwords, punctuations, and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuations = set(string.punctuation)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and punctuations, and lowercase the words\n",
        "    words = [word.lower() for word in words if word.lower() not in stop_words and word not in punctuations]\n",
        "\n",
        "    # Apply stemming\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Join the words back into a single string\n",
        "    preprocessed_text = ' '.join(words)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply text preprocessing to the input_text column\n",
        "train_data['input_text'] = train_data['input_text'].apply(preprocess_text)\n",
        "\n",
        "# Define input and target variables\n",
        "X = train_data['input_text'].values\n",
        "y = train_data['label'].values\n",
        "\n",
        "# Tokenize the text data with a limited vocabulary size\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "X_sequences = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Pad sequences to ensure uniform length for LSTM input\n",
        "max_sequence_length = 100  # Adjust this value as needed\n",
        "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Get the vocabulary size (number of unique words in the tokenizer)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 64, input_length=max_sequence_length))  # Lower the embedding dimension (e.g., 64)\n",
        "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))  # Lower the LSTM dimension (e.g., 32)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Data generator for training to load smaller batches\n",
        "def data_generator(X, y, batch_size):\n",
        "    steps = len(X) // batch_size\n",
        "    while True:\n",
        "        for i in range(steps):\n",
        "            batch_X = X[i * batch_size: (i + 1) * batch_size]\n",
        "            batch_y = y[i * batch_size: (i + 1) * batch_size]\n",
        "            yield batch_X, batch_y\n",
        "\n",
        "# Train the model using the data generator\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "steps_per_epoch = len(X_train) // batch_size\n",
        "model.fit(data_generator(X_train, y_train, batch_size), steps_per_epoch=steps_per_epoch,\n",
        "          validation_data=(X_val, y_val), epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMLby2tO9_nj",
        "outputId": "89b22178-3eb4-46a9-989a-550f8b61fc33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163/163 [==============================] - 5s 29ms/step\n",
            "Model Accuracy on Test Data: 0.5478846153846154\n"
          ]
        }
      ],
      "source": [
        "# Load the submit data\n",
        "submit_path = \"/content/submit.csv\"\n",
        "submit_data = pd.read_csv(submit_path)\n",
        "\n",
        "# Load and preprocess the test data\n",
        "test_path = \"/content/test.csv\"\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Combine 'title', 'author', and 'text' columns to create a single input text\n",
        "test_data['input_text'] = test_data['title'].fillna('') + ' ' + test_data['author'].fillna('') + ' ' + test_data['text'].fillna('')\n",
        "\n",
        "# Apply the same text preprocessing to the test data\n",
        "X_test = test_data['input_text'].values\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_test_pred = model.predict(X_test_padded)\n",
        "y_test_pred_labels = np.round(y_test_pred).astype(int).flatten()\n",
        "\n",
        "# Store the results in a DataFrame\n",
        "results_df = pd.DataFrame({'id': test_data['id'], 'label': y_test_pred_labels, 'original_label': submit_data['label']})\n",
        "\n",
        "# Save the results to a new CSV file named 'results.csv'\n",
        "results_path = \"/content/results.csv\"\n",
        "results_df.to_csv(results_path, index=False)\n",
        "\n",
        "# Compare the predictions with the values in submit.csv\n",
        "accuracy = np.mean(results_df['label'] == submit_data['label'])\n",
        "\n",
        "print(\"Model Accuracy on Test Data:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb5WxT-tGLZK",
        "outputId": "1ad58f04-8ed0-4b8b-a68e-55af6f98d840"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 0.5478846153846154\n",
            "Precision: 0.5493039443155452\n",
            "Recall: 0.9930094372596994\n",
            "F1-Score: 0.707332254450392\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the submit data (ground truth labels)\n",
        "submit_path = \"/content/submit.csv\"\n",
        "submit_data = pd.read_csv(submit_path)\n",
        "\n",
        "# Compare the predictions with the ground truth labels\n",
        "accuracy = accuracy_score(submit_data['label'], results_df['label'])\n",
        "precision = precision_score(submit_data['label'], results_df['label'])\n",
        "recall = recall_score(submit_data['label'], results_df['label'])\n",
        "f1 = f1_score(submit_data['label'], results_df['label'])\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zG42qcsY_uda"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model_path = \"/content/model.LSTM\"\n",
        "model.save(model_path)\n",
        "\n",
        "# Save the tokenizer\n",
        "import pickle\n",
        "\n",
        "tokenizer_path = \"/content/tokenizer.LSTM\"\n",
        "with open(tokenizer_path, 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK7px4Rq_yEH",
        "outputId": "9ad74407-5fb4-4dd6-bfb0-d35595d06aa5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Load the tokenizer\n",
        "with open(tokenizer_path, 'rb') as handle:\n",
        "    loaded_tokenizer = pickle.load(handle)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "aUlZ7IacCM5a",
        "outputId": "38abb2ed-3ee4-4e11-d092-4c6ad9928cde"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_e28aa16a-7ea2-44f8-83a2-33483ed8f3c9\", \"model.LSTM\", 4096)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/model.LSTM')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jipc0bStGZii",
        "outputId": "044ec536-91e5-4eac-d589-b7c04112e8c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 255ms/step\n",
            "This text is likely to be fake.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the model\n",
        "model_path = \"/content/model.LSTM\"\n",
        "loaded_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer_path = \"/content/tokenizer.LSTM\"\n",
        "with open(tokenizer_path, 'rb') as handle:\n",
        "    loaded_tokenizer = pickle.load(handle)\n",
        "\n",
        "# Define the new text\n",
        "new_text = \"\"\n",
        "\n",
        "# Preprocess the new text\n",
        "preprocessed_text = preprocess_text(new_text)  # using the same preprocess_text function you defined during training\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "new_text_sequence = loaded_tokenizer.texts_to_sequences([preprocessed_text])\n",
        "\n",
        "# Pad sequences to ensure uniform length for LSTM input\n",
        "max_sequence_length = 100  # same as during training\n",
        "new_text_padded = pad_sequences(new_text_sequence, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Make a prediction\n",
        "prediction = loaded_model.predict(new_text_padded)\n",
        "\n",
        "# Print the result\n",
        "if prediction >= 0.5:\n",
        "    print(\"This text is likely to be fake.\")\n",
        "else:\n",
        "    print(\"This text is likely to be real.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "KQJUy-i7KeVN",
        "outputId": "f2fd6334-e0ea-4666-98f4-b1723fc5b313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "/content/model.h5\n",
            "tar: Removing leading `/' from member names\n",
            "/content/tokenizer.pickle\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_a2c114a9-70c2-458b-81b9-2a7064b17b6b\", \"model.tar.gz\", 42371783)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_5cd73691-c349-4155-a27d-5b1e31a60927\", \"tokenizer.tar.gz\", 3093533)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!tar czvf model.tar.gz /content/model.h5\n",
        "!tar czvf tokenizer.tar.gz /content/tokenizer.pickle\n",
        "\n",
        "# Download the compressed files\n",
        "files.download('/content/model.tar.gz')\n",
        "files.download('/content/tokenizer.tar.gz')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
