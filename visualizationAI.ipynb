{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxKuS9Col9Vr",
        "outputId": "08cf39d0-eb95-4615-aa50-739febdbafd3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file paths\n",
        "file_paths = [\n",
        "    \"/content/GNN_results.csv\",\n",
        "    \"/content/logistic_results.csv\",\n",
        "    \"/content/predicted_submit.csv\",\n",
        "    \"/content/test.csv\"\n",
        "]\n",
        "\n",
        "# Loop through each file and print the column names\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Column names for {file_path}:\")\n",
        "    print(df.columns.tolist())\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2SFB824m0r1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load each CSV file into a DataFrame\n",
        "df_predicted_submit = pd.read_csv(\"/content/predicted_submit.csv\")\n",
        "df_logistic = pd.read_csv(\"/content/logistic_results.csv\")\n",
        "df_gnn = pd.read_csv(\"/content/GNN_results.csv\")\n",
        "\n",
        "# Merge the DataFrames based on the \"id\" column\n",
        "df_merge_1 = pd.merge(df_predicted_submit[['id', 'predicted_label', 'label']], df_logistic[['id', 'predicted_label']], on='id', suffixes=('_LSTM', '_Logistic'))\n",
        "df_final = pd.merge(df_merge_1, df_gnn[['id', 'predicted_label']], on='id')\n",
        "\n",
        "# Rename columns\n",
        "df_final.rename(columns={\n",
        "    'predicted_label_LSTM': 'LSTM_prediction',\n",
        "    'predicted_label_Logistic': 'Logistic_prediction',\n",
        "    'predicted_label': 'GNN_prediction',\n",
        "    'label': 'real_label'\n",
        "}, inplace=True)\n",
        "\n",
        "# Save the final DataFrame to a new CSV file\n",
        "df_final.to_csv(\"/content/prediction_summary.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cw0NYySoGYD",
        "outputId": "2b3da48d-a8e6-4e03-b346-8e9cb268fca9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df_summary = pd.read_csv(\"/content/prediction_summary.csv\")\n",
        "\n",
        "# Print the column names\n",
        "print(df_summary.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9a7X53PoPBT",
        "outputId": "297bf884-c871-4484-8eab-66c0f7137e15"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df_summary = pd.read_csv(\"/content/prediction_summary.csv\")\n",
        "\n",
        "# Compute accuracy for LSTM model\n",
        "lstm_accuracy = (df_summary['LSTM_prediction'] == df_summary['real_label']).mean()\n",
        "print(f\"LSTM Model Accuracy: {lstm_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Compute accuracy for Logistic model\n",
        "logistic_accuracy = (df_summary['Logistic_prediction'] == df_summary['real_label']).mean()\n",
        "print(f\"Logistic Regression Model Accuracy: {logistic_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Compute accuracy for GNN model\n",
        "gnn_accuracy = (df_summary['GNN_prediction'] == df_summary['real_label']).mean()\n",
        "print(f\"GNN Model Accuracy: {gnn_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "RD3FupnQof38",
        "outputId": "ae3cde56-e6e0-4ddf-c32c-61b1c0ddf072"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "models = ['LSTM', 'Logistic Regression', 'GNN']\n",
        "accuracies = [61.10, 63.58, 63.25]\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, accuracies, color=['blue', 'green', 'red'])\n",
        "\n",
        "# Title and labels\n",
        "plt.title('Model Accuracies Comparison')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Models')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "Yzhopkoo0kZR",
        "outputId": "797e7212-7582-41a0-b1ae-a6d6f14dbdec"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Extract the real labels\n",
        "real_labels = df_summary['real_label']\n",
        "\n",
        "# Define a list for the prediction columns\n",
        "prediction_columns = ['LSTM_prediction', 'Logistic_prediction', 'GNN_prediction']\n",
        "\n",
        "# Calculate and print the metrics for each model\n",
        "for column in prediction_columns:\n",
        "    predictions = df_summary[column]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(real_labels, predictions) * 100  # Convert to percentage\n",
        "    precision = precision_score(real_labels, predictions, average='macro', zero_division=0)\n",
        "    recall = recall_score(real_labels, predictions, average='macro', zero_division=0)\n",
        "    f1 = f1_score(real_labels, predictions, average='macro', zero_division=0)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Metrics for {column.replace('_prediction', '')}:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "kbFFI1Aj0-ua",
        "outputId": "fd521ea2-d6c3-47db-e2eb-83825eac7980"
      },
      "outputs": [],
      "source": [
        "# Check NaN count for each prediction column\n",
        "for column in prediction_columns:\n",
        "    nan_count = df_summary[column].isna().sum()\n",
        "    if nan_count > 0:\n",
        "        print(f\"{column} has {nan_count} NaN values.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nchVcnwD1QEh"
      },
      "outputs": [],
      "source": [
        "# Drop rows where any of the prediction columns or the real_label column has NaN values\n",
        "df_summary_clean = df_summary.dropna(subset=prediction_columns + ['real_label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kfnp-ou20kN",
        "outputId": "dde11101-392a-4a66-d8dd-564728bd4a84"
      },
      "outputs": [],
      "source": [
        "# Extract the real labels from cleaned data\n",
        "real_labels_clean = df_summary_clean['real_label']\n",
        "\n",
        "# Calculate and print the metrics for each model\n",
        "for column in prediction_columns:\n",
        "    predictions = df_summary_clean[column]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(real_labels_clean, predictions) * 100  # Convert to percentage\n",
        "    precision = precision_score(real_labels_clean, predictions, average='macro', zero_division=0)\n",
        "    recall = recall_score(real_labels_clean, predictions, average='macro', zero_division=0)\n",
        "    f1 = f1_score(real_labels_clean, predictions, average='macro', zero_division=0)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Metrics for {column.replace('_prediction', '')}:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L31kM9Er23kW",
        "outputId": "d3b7537d-1eb1-49c1-f6ef-44915be848a7"
      },
      "outputs": [],
      "source": [
        "print(df_summary_clean.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP7m0tfi3SDX",
        "outputId": "88ef4946-670c-4722-b4a9-9781a349081a"
      },
      "outputs": [],
      "source": [
        "for column in prediction_columns + ['real_label']:\n",
        "    print(f\"Unique values in {column}: {df_summary_clean[column].unique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcGXb-4n3bLp",
        "outputId": "366554d7-8e1f-4f1b-f9ea-d80eb81010ce"
      },
      "outputs": [],
      "source": [
        "print(f\"Original dataframe length: {len(df_summary)}\")\n",
        "print(f\"Cleaned dataframe length: {len(df_summary_clean)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5JPNzcX3fOP"
      },
      "outputs": [],
      "source": [
        "labels = df_summary_clean['real_label'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYGcNsCq3j47",
        "outputId": "1941f69e-7b98-4d17-b37b-5f97b105323f"
      },
      "outputs": [],
      "source": [
        "for column in prediction_columns:\n",
        "    predictions = df_summary_clean[column]\n",
        "\n",
        "    # Calculate metrics with explicit labels\n",
        "    accuracy = accuracy_score(real_labels_clean, predictions) * 100\n",
        "    precision = precision_score(real_labels_clean, predictions, average='macro', labels=labels, zero_division=0)\n",
        "    recall = recall_score(real_labels_clean, predictions, average='macro', labels=labels, zero_division=0)\n",
        "    f1 = f1_score(real_labels_clean, predictions, average='macro', labels=labels, zero_division=0)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Metrics for {column.replace('_prediction', '')}:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf5gWkVU3llg",
        "outputId": "295f02fc-4f45-406f-f834-9b1b7936f4ff"
      },
      "outputs": [],
      "source": [
        "# Create a subset dataframe from the provided data\n",
        "data_subset = {\n",
        "    'id': [20800, 20801, 20802, 20803, 20804, 20805, 20806, 20807, 20808, 20809],\n",
        "    'LSTM_prediction': [0, 1, 1, 0, 1, 1, 0, 1, 1, 1],\n",
        "    'real_label': [0, 1, 0, 1, 1, 1, 1, 1, 0, 1],\n",
        "    'Logistic_prediction': [0, 1, 1, 0, 1, 1, 0, 1, 1, 1],\n",
        "    'GNN_prediction': [0, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
        "}\n",
        "\n",
        "df_subset = pd.DataFrame(data_subset)\n",
        "\n",
        "# Extract the real labels from the subset\n",
        "real_labels_subset = df_subset['real_label']\n",
        "\n",
        "# Calculate and print the metrics for each model\n",
        "for column in prediction_columns:\n",
        "    predictions = df_subset[column]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(real_labels_subset, predictions) * 100\n",
        "    precision = precision_score(real_labels_subset, predictions, average='macro', zero_division=0)\n",
        "    recall = recall_score(real_labels_subset, predictions, average='macro', zero_division=0)\n",
        "    f1 = f1_score(real_labels_subset, predictions, average='macro', zero_division=0)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Metrics for {column.replace('_prediction', '')}:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvdW0rqt3pzl",
        "outputId": "d6d9ec35-de91-4815-d36f-159e85bb6325"
      },
      "outputs": [],
      "source": [
        "print(df_summary_clean[['LSTM_prediction', 'Logistic_prediction', 'GNN_prediction']].corr())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQl1FHUq3_gD",
        "outputId": "70e25ea9-caad-483f-bb5a-06668302161b"
      },
      "outputs": [],
      "source": [
        "lstm_logistic_matches = (df_summary_clean['LSTM_prediction'] == df_summary_clean['Logistic_prediction']).sum()\n",
        "lstm_gnn_matches = (df_summary_clean['LSTM_prediction'] == df_summary_clean['GNN_prediction']).sum()\n",
        "logistic_gnn_matches = (df_summary_clean['Logistic_prediction'] == df_summary_clean['GNN_prediction']).sum()\n",
        "\n",
        "print(f\"Number of LSTM and Logistic matches: {lstm_logistic_matches}\")\n",
        "print(f\"Number of LSTM and GNN matches: {lstm_gnn_matches}\")\n",
        "print(f\"Number of Logistic and GNN matches: {logistic_gnn_matches}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN0uObZe4Dl5",
        "outputId": "532f8caa-967d-4247-bd65-8f626fcac1ff"
      },
      "outputs": [],
      "source": [
        "# LSTM gets it right, but Logistic and GNN get it wrong\n",
        "lstm_right = ((df_summary['LSTM_prediction'] == df_summary['real_label']) &\n",
        "             (df_summary['Logistic_prediction'] != df_summary['real_label']) &\n",
        "             (df_summary['GNN_prediction'] != df_summary['real_label']))\n",
        "\n",
        "# Logistic gets it right, but LSTM and GNN get it wrong\n",
        "logistic_right = ((df_summary['Logistic_prediction'] == df_summary['real_label']) &\n",
        "                 (df_summary['LSTM_prediction'] != df_summary['real_label']) &\n",
        "                 (df_summary['GNN_prediction'] != df_summary['real_label']))\n",
        "\n",
        "# GNN gets it right, but LSTM and Logistic get it wrong\n",
        "gnn_right = ((df_summary['GNN_prediction'] == df_summary['real_label']) &\n",
        "            (df_summary['LSTM_prediction'] != df_summary['real_label']) &\n",
        "            (df_summary['Logistic_prediction'] != df_summary['real_label']))\n",
        "\n",
        "# Print the counts\n",
        "print(f\"LSTM got right while others got wrong: {lstm_right.sum()}\")\n",
        "print(f\"Logistic got right while others got wrong: {logistic_right.sum()}\")\n",
        "print(f\"GNN got right while others got wrong: {gnn_right.sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "nAF3GXjm4pOM",
        "outputId": "a2f8790d-5694-4d98-8a4a-f4fd00bb4b6b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Data\n",
        "models = ['LSTM', 'Logistic Regression', 'GNN']\n",
        "correct_predictions = [99, 73, 55]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.bar(models, correct_predictions, color=['blue', 'green', 'red'])\n",
        "\n",
        "# Add labels and title\n",
        "plt.ylabel('Number of Correct Predictions')\n",
        "plt.title('Number of Correct Predictions Unique to Each Model')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "32CvMqLnf5nX",
        "outputId": "30103120-d4cd-4b69-c588-5fbc38827fe1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Given metrics in your example\n",
        "models = ['LSTM', 'Logistic', 'GNN']\n",
        "accuracy_values = [60, 60, 60]\n",
        "precision_values = [0.52, 0.52, 0.52]\n",
        "recall_values = [0.52, 0.52, 0.52]\n",
        "f1_values = [0.52, 0.52, 0.52]\n",
        "\n",
        "barWidth = 0.2\n",
        "r1 = np.arange(len(accuracy_values))\n",
        "r2 = [x + barWidth for x in r1]\n",
        "r3 = [x + barWidth for x in r2]\n",
        "r4 = [x + barWidth for x in r3]\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Create bars\n",
        "plt.bar(r1, accuracy_values, width=barWidth, color='b', edgecolor='grey', label='Accuracy (%)')\n",
        "plt.bar(r2, np.array(precision_values) * 100, width=barWidth, color='c', edgecolor='grey', label='Precision x100')\n",
        "plt.bar(r3, np.array(recall_values) * 100, width=barWidth, color='m', edgecolor='grey', label='Recall x100')\n",
        "plt.bar(r4, np.array(f1_values) * 100, width=barWidth, color='r', edgecolor='grey', label='F1 Score x100')\n",
        "\n",
        "# Title & Subtitle\n",
        "plt.title('Model Performance Metrics', fontweight='bold')\n",
        "plt.xlabel('Model', fontweight='bold')\n",
        "plt.xticks([r + barWidth for r in range(len(accuracy_values))], models)\n",
        "\n",
        "# Create legend & Show graphic\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK_gP_EfiOQM"
      },
      "outputs": [],
      "source": [
        "data_subset = {\n",
        "    'id': [20800, 20801, 20802, 20803, 20804, 20805, 20806, 20807, 20808, 20809],\n",
        "    'LSTM_prediction': [0, 1, 1, 0, 1, 1, 0, 1, 1, 1],\n",
        "    'real_label': [0, 1, 0, 1, 1, 1, 1, 1, 0, 1],\n",
        "    'Logistic_prediction': [0, 1, 1, 0, 1, 1, 0, 1, 1, 1],\n",
        "    'GNN_prediction': [0, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
        "}\n",
        "df_subset = pd.DataFrame(data_subset)\n",
        "real_labels_subset = df_subset['real_label']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "XDTEjSdzhM1D",
        "outputId": "ce2968c1-54a5-4d85-fa93-8ce70cf69a82"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Function to extract TP, TN, FP, FN\n",
        "def get_confusion_metrics(real, pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(real, pred).ravel()\n",
        "    return tp, tn, fp, fn\n",
        "\n",
        "models = ['LSTM', 'Logistic', 'GNN']\n",
        "prediction_columns = ['LSTM_prediction', 'Logistic_prediction', 'GNN_prediction']\n",
        "\n",
        "tp_values = []\n",
        "tn_values = []\n",
        "fp_values = []\n",
        "fn_values = []\n",
        "\n",
        "# Calculate TP, TN, FP, FN for each model\n",
        "for column in prediction_columns:\n",
        "    predictions = df_subset[column]\n",
        "    tp, tn, fp, fn = get_confusion_metrics(real_labels_subset, predictions)\n",
        "\n",
        "    tp_values.append(tp)\n",
        "    tn_values.append(tn)\n",
        "    fp_values.append(fp)\n",
        "    fn_values.append(fn)\n",
        "\n",
        "barWidth = 0.2\n",
        "r1 = np.arange(len(tp_values))\n",
        "r2 = [x + barWidth for x in r1]\n",
        "r3 = [x + barWidth for x in r2]\n",
        "r4 = [x + barWidth for x in r3]\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Create bars\n",
        "plt.bar(r1, tp_values, width=barWidth, color='b', edgecolor='grey', label='True Positives')\n",
        "plt.bar(r2, tn_values, width=barWidth, color='c', edgecolor='grey', label='True Negatives')\n",
        "plt.bar(r3, fp_values, width=barWidth, color='m', edgecolor='grey', label='False Positives')\n",
        "plt.bar(r4, fn_values, width=barWidth, color='r', edgecolor='grey', label='False Negatives')\n",
        "\n",
        "# Title & Subtitle\n",
        "plt.title('Model Confusion Metrics', fontweight='bold')\n",
        "plt.xlabel('Model', fontweight='bold')\n",
        "plt.xticks([r + barWidth for r in range(len(tp_values))], models)\n",
        "\n",
        "# Add text labels on top of the bars\n",
        "for i in range(len(r1)):\n",
        "    plt.text(r1[i], tp_values[i] + 0.1, str(tp_values[i]), ha='center')\n",
        "    plt.text(r2[i], tn_values[i] + 0.1, str(tn_values[i]), ha='center')\n",
        "    plt.text(r3[i], fp_values[i] + 0.1, str(fp_values[i]), ha='center')\n",
        "    plt.text(r4[i], fn_values[i] + 0.1, str(fn_values[i]), ha='center')\n",
        "\n",
        "# Create legend & Show graphic\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfa7WPcgh_I8",
        "outputId": "bc382a0c-1f38-47d9-c442-d893cfb88523"
      },
      "outputs": [],
      "source": [
        "# Print the number of rows/samples in the dataframe\n",
        "print(f\"We are working on {df_subset.shape[0]} rows/samples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJc1ramQi4x2",
        "outputId": "9653d164-b8fe-4602-f369-52e55ce2a560"
      },
      "outputs": [],
      "source": [
        "# Print the number of rows/samples in the dataframe\n",
        "print(f\"We are working on {df_summary.shape[0]} rows/samples to calculate accuracies.\")\n",
        "\n",
        "# Your existing code for calculating accuracies, true positives, true negatives, etc. will follow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP4N7LD9jJJR",
        "outputId": "5261ef60-7a4e-483b-b468-86a5caf71bbd"
      },
      "outputs": [],
      "source": [
        "print(df_summary.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "VaH-lE4tjTGJ",
        "outputId": "2fe13c27-7415-48b8-9a96-20a4f15da278"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculate the accuracy for LSTM\n",
        "lstm_accuracy = accuracy_score(df_summary['real_label'], df_summary['LSTM_prediction'])\n",
        "print(f\"LSTM Accuracy: {lstm_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Calculate the accuracy for Logistic Regression\n",
        "logistic_accuracy = accuracy_score(df_summary['real_label'], df_summary['Logistic_prediction'])\n",
        "print(f\"Logistic Regression Accuracy: {logistic_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Calculate the accuracy for GNN\n",
        "gnn_accuracy = accuracy_score(df_summary['real_label'], df_summary['GNN_prediction'])\n",
        "print(f\"GNN Accuracy: {gnn_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpMl8dLZju4H",
        "outputId": "b69e2d0e-86d2-46a5-c3a3-ff61b0e416cd"
      },
      "outputs": [],
      "source": [
        "# Remove rows with NaN values in the prediction columns\n",
        "df_clean = df_summary.dropna(subset=['LSTM_prediction', 'Logistic_prediction', 'GNN_prediction'])\n",
        "\n",
        "# Calculate the accuracy for LSTM\n",
        "lstm_accuracy = accuracy_score(df_clean['real_label'], df_clean['LSTM_prediction'])\n",
        "print(f\"LSTM Accuracy: {lstm_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Calculate the accuracy for Logistic Regression\n",
        "logistic_accuracy = accuracy_score(df_clean['real_label'], df_clean['Logistic_prediction'])\n",
        "print(f\"Logistic Regression Accuracy: {logistic_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Calculate the accuracy for GNN\n",
        "gnn_accuracy = accuracy_score(df_clean['real_label'], df_clean['GNN_prediction'])\n",
        "print(f\"GNN Accuracy: {gnn_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XShHYD18j63p"
      },
      "outputs": [],
      "source": [
        "df_clean.to_csv('prediction_summary2.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ofn4KC1kkK3B",
        "outputId": "4309b454-0c76-4774-f305-401ddb2b7fb0"
      },
      "outputs": [],
      "source": [
        "# Load the cleaned data\n",
        "df_cleaned = pd.read_csv('prediction_summary2.csv')\n",
        "\n",
        "# Find out the number of rows\n",
        "num_rows = df_cleaned.shape[0]\n",
        "\n",
        "print(f\"We are working on {num_rows} rows.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCGTArOvkTYV",
        "outputId": "c38f564f-944d-432e-94e6-604d276f3c68"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Extract the real labels and logistic predictions\n",
        "real_labels = df_cleaned['real_label']\n",
        "logistic_predictions = df_cleaned['Logistic_prediction']\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(real_labels, logistic_predictions)\n",
        "precision = precision_score(real_labels, logistic_predictions, average='binary')\n",
        "recall = recall_score(real_labels, logistic_predictions, average='binary')\n",
        "f1 = f1_score(real_labels, logistic_predictions, average='binary')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Metrics for Logistic Prediction:\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59coPcFdkxjq",
        "outputId": "92363603-7fd3-432f-83aa-49eefc32ce76"
      },
      "outputs": [],
      "source": [
        "# Extract the LSTM predictions\n",
        "lstm_predictions = df_cleaned['LSTM_prediction']\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(real_labels, lstm_predictions)\n",
        "precision = precision_score(real_labels, lstm_predictions, average='binary')\n",
        "recall = recall_score(real_labels, lstm_predictions, average='binary')\n",
        "f1 = f1_score(real_labels, lstm_predictions, average='binary')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Metrics for LSTM Prediction:\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWZPaSQ9lAVh",
        "outputId": "6f830980-fc42-4345-d901-2cad41d2036d"
      },
      "outputs": [],
      "source": [
        "# Extract the GNN predictions\n",
        "gnn_predictions = df_cleaned['GNN_prediction']\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(real_labels, gnn_predictions)\n",
        "precision = precision_score(real_labels, gnn_predictions, average='binary')\n",
        "recall = recall_score(real_labels, gnn_predictions, average='binary')\n",
        "f1 = f1_score(real_labels, gnn_predictions, average='binary')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Metrics for GNN Prediction:\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "3sYC8XGElPEs",
        "outputId": "da2ce9a7-a931-4108-b065-bc2e4423dcbd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metrics to plot\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "values = [accuracy, precision, recall, f1]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics, values, color=['blue', 'green', 'red', 'cyan'])\n",
        "plt.ylim(0, 1)  # Setting the y-axis limits to be between 0 and 1 since our metrics are in that range\n",
        "plt.title('Metrics for Logistic Prediction')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Metrics')\n",
        "\n",
        "# Displaying the value of each metric on top of each bar\n",
        "for i, v in enumerate(values):\n",
        "    plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "xmv48OGqmMLk",
        "outputId": "20894766-7018-4c41-9b6a-f67eb5d9bfc9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Given metrics for LSTM\n",
        "accuracy_lstm = 0.6118\n",
        "precision_lstm = 0.67\n",
        "recall_lstm = 0.58\n",
        "f1_lstm = 0.62\n",
        "\n",
        "# Lists to hold metrics and their values for LSTM\n",
        "metrics_lstm = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "values_lstm = [accuracy_lstm, precision_lstm, recall_lstm, f1_lstm]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics_lstm, values_lstm, color=['blue', 'green', 'red', 'cyan'])\n",
        "plt.ylim(0, 1)  # Setting the y-axis limits to be between 0 and 1 since our metrics are in that range\n",
        "plt.title('Metrics for LSTM Prediction')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Metrics')\n",
        "\n",
        "# Displaying the value of each metric on top of each bar\n",
        "for i, v in enumerate(values_lstm):\n",
        "    plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "t0agPGD8myxC",
        "outputId": "a75e5d2a-7ea3-419c-c6fe-81f2d75b5841"
      },
      "outputs": [],
      "source": [
        "# Given metrics for GNN (from the provided values)\n",
        "accuracy_gnn = accuracy\n",
        "precision_gnn = precision\n",
        "recall_gnn = recall\n",
        "f1_gnn = f1\n",
        "\n",
        "# Lists to hold metrics and their values for GNN\n",
        "metrics_gnn = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "values_gnn = [accuracy_gnn, precision_gnn, recall_gnn, f1_gnn]\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics_gnn, values_gnn, color=['blue', 'green', 'red', 'cyan'])\n",
        "plt.ylim(0, 1)  # Setting the y-axis limits to be between 0 and 1 since our metrics are in that range\n",
        "plt.title('Metrics for GNN Prediction')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Metrics')\n",
        "\n",
        "# Displaying the value of each metric on top of each bar\n",
        "for i, v in enumerate(values_gnn):\n",
        "    plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ppMyjGWxotk9",
        "outputId": "c7968f35-2d4b-4189-aa81-cbef6c07ded6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define a function to plot confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Negative', 'Positive'],\n",
        "                yticklabels=['Negative', 'Positive'])\n",
        "\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrix for each model\n",
        "plot_confusion_matrix(df_cleaned['real_label'], df_cleaned['Logistic_prediction'], \"Confusion Matrix for Logistic Prediction\")\n",
        "plot_confusion_matrix(df_cleaned['real_label'], df_cleaned['LSTM_prediction'], \"Confusion Matrix for LSTM Prediction\")\n",
        "plot_confusion_matrix(df_cleaned['real_label'], df_cleaned['GNN_prediction'], \"Confusion Matrix for GNN Prediction\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOzPI1CXqOj5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define a function to plot and save confusion matrix\n",
        "def save_confusion_matrix(y_true, y_pred, title, filename):\n",
        "    matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Negative', 'Positive'],\n",
        "                yticklabels=['Negative', 'Positive'])\n",
        "\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()  # This ensures that labels are not cut off when saving\n",
        "    plt.savefig(filename)  # Save the image\n",
        "    plt.close()  # Close the current figure\n",
        "\n",
        "# Save confusion matrix image for Logistic\n",
        "save_confusion_matrix(df_cleaned['real_label'], df_cleaned['Logistic_prediction'], \"Confusion Matrix for Logistic Prediction\", \"logistic_cm.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWniFEryql1A"
      },
      "outputs": [],
      "source": [
        "# Save confusion matrix image for LSTM\n",
        "save_confusion_matrix(df_cleaned['real_label'], df_cleaned['LSTM_prediction'], \"Confusion Matrix for LSTM Prediction\", \"lstm_cm.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3RFOhbxr_rO"
      },
      "outputs": [],
      "source": [
        "# Save confusion matrix image for GNN\n",
        "save_confusion_matrix(df_cleaned['real_label'], df_cleaned['GNN_prediction'], \"Confusion Matrix for GNN Prediction\", \"gnn_cm.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er6yeisvsieA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
